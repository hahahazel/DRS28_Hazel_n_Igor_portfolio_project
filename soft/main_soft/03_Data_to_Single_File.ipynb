{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## environment preparation\n",
    "##============================\n",
    "\n",
    "%reset -f\n",
    "\n",
    "%connect_info #might be used for \"jupyter-console\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "##============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change to the directory with data\n",
    "##============================\n",
    "\n",
    "#Change the directory here:\n",
    "dir_data = \"demo_run/\"\n",
    "\n",
    "path_ipynb = os.getcwd()\n",
    "path_base = path_ipynb + '/../../data/readback/'\n",
    "pathW = path_base + dir_data\n",
    "os.chdir(pathW)\n",
    "print(os.getcwd())\n",
    "\n",
    "path_2save = path_ipynb + '/../../data/processed_data/'\n",
    "\n",
    "files_list = os.listdir(pathW)\n",
    "files_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets see the files with data\n",
    "##============================\n",
    "\n",
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count max length of lines in the files\n",
    "# for balancing afterwards\n",
    "# linux shell cmd is used\n",
    "list_of_flines= !wc -l {pathW+\"*\"} #get list of n_of lines in each file\n",
    "list_of_flines = list(list_of_flines) #convert to usual list\n",
    "# -1 bcoz last element is sum of all n_of lines\n",
    "max_of_flines = max([int(list_element.split()[0]) for list_element in list_of_flines][:-1]) - 1\n",
    "max_of_flines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93699cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsampling and Balancing\n",
    "##============================\n",
    "\n",
    "## Change upsample_ratio to make more data based on existing.\n",
    "## It is possible to select the distribution on which \n",
    "## the new posint will be created \"Uniform\" or \"Normal\" augmentation.\n",
    "## To select the distrubition uncomment/comment necessary part in this cell.\n",
    "upsample_ratio = 1\n",
    "\n",
    "Xdf = pd.DataFrame([], columns=[\"time_ms\", \n",
    "                                \"B102NO2\", \"B302C2H5OH\", \"B502VOC\", \"B702CO\", \n",
    "                                \"TdegC\", \"RH\", \n",
    "                                \"label\", \"add_label\", \"condition\"])\n",
    "\n",
    "for fileN in files_list:\n",
    "    file_info = fileN.split(\"_\")\n",
    "    label_c = file_info[0]\n",
    "    add_label_c = file_info[1]\n",
    "    condition_c = file_info[2]\n",
    "    \n",
    "    df_c0 = pd.read_csv(fileN)\n",
    "\n",
    "    \n",
    "##############################################    \n",
    "    n_of_add_samples = int(max_of_flines * upsample_ratio - df_c0.shape[0])\n",
    "\n",
    "\n",
    "    time_ms_t    = np.linspace(start=df_c0.time_ms.max(), stop=df_c0.time_ms.max()+n_of_add_samples*100, num=n_of_add_samples, dtype=int)\n",
    "    \n",
    "    ##Uniform augmentation\n",
    "    ##format: uniform(low=0.0, high=1.0, size=None)\n",
    "#     B102NO2_t    = np.random.uniform(low=df_c0.B102NO2.min(), high=df_c0.B102NO2.max(), size=n_of_add_samples)\n",
    "#     B302C2H5OH_t = np.random.uniform(low=df_c0.B302C2H5OH.min(), high=df_c0.B302C2H5OH.max(), size=n_of_add_samples)\n",
    "#     B502VOC_t    = np.random.uniform(low=df_c0.B502VOC.min(), high=df_c0.B502VOC.max(), size=n_of_add_samples)\n",
    "#     B702CO_t     = np.random.uniform(low=df_c0.B702CO.min(), high=df_c0.B702CO.max(), size=n_of_add_samples)\n",
    "#     TdegC_t      = np.random.uniform(low=df_c0.TdegC.min(), high=df_c0.TdegC.max(), size=n_of_add_samples)\n",
    "#     RH_t         = np.random.uniform(low=df_c0.RH.min(), high=df_c0.RH.max(), size=n_of_add_samples)\n",
    "    \n",
    "    ##Normal augmentation\n",
    "    ##format: normal(loc=0.0, scale=1.0, size=None)\n",
    "    B102NO2_t    = np.random.normal(loc=df_c0.B102NO2.median(), scale=df_c0.B102NO2.std(), size=n_of_add_samples)\n",
    "    B302C2H5OH_t = np.random.normal(loc=df_c0.B302C2H5OH.median(), scale=df_c0.B302C2H5OH.std(), size=n_of_add_samples)\n",
    "    B502VOC_t    = np.random.normal(loc=df_c0.B502VOC.median(), scale=df_c0.B502VOC.std(), size=n_of_add_samples)\n",
    "    B702CO_t     = np.random.normal(loc=df_c0.B702CO.median(), scale=df_c0.B702CO.std(), size=n_of_add_samples)\n",
    "    TdegC_t      = np.random.normal(loc=df_c0.TdegC.median(), scale=df_c0.TdegC.std(), size=n_of_add_samples)\n",
    "    RH_t         = np.random.normal(loc=df_c0.RH.median(), scale=df_c0.RH.std(), size=n_of_add_samples)\n",
    "    \n",
    "    \n",
    "    time_ms_t.shape    = (time_ms_t.shape[0],1)\n",
    "    B102NO2_t.shape    = (B102NO2_t.shape[0],1)\n",
    "    B302C2H5OH_t.shape = (B302C2H5OH_t.shape[0],1)\n",
    "    B502VOC_t.shape    = (B502VOC_t.shape[0],1)\n",
    "    B702CO_t.shape     = (B702CO_t.shape[0],1)\n",
    "    TdegC_t.shape      = (TdegC_t.shape[0],1)\n",
    "    RH_t.shape         = (RH_t.shape[0],1)\n",
    "\n",
    "    df_c0u = pd.concat([df_c0, \n",
    "                        pd.DataFrame(np.hstack(\n",
    "                            (time_ms_t, B102NO2_t, B302C2H5OH_t, B502VOC_t, B702CO_t, \n",
    "                             TdegC_t, RH_t)), columns=df_c0.columns)], \n",
    "                       axis=0)\\\n",
    "            .reset_index(drop=True)\n",
    "##############################################\n",
    "\n",
    "\n",
    "    feature_add_dict = {\n",
    "        \"label\": label_c,\n",
    "        \"add_label\": add_label_c,\n",
    "        \"condition\": condition_c,\n",
    "    } \n",
    "    df_c_upd = pd.concat([df_c0u, pd.DataFrame(feature_add_dict, index=df_c0u.index)], axis=1)\n",
    "    Xdf = pd.concat([Xdf, df_c_upd], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a150816",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to a new file\n",
    "##============================\n",
    "\n",
    "## line7: \"_normal\" or \"_uniform\" depending of the selected algorythm before\n",
    "filename_to_save = dir_data[:-1] + '_' +\\\n",
    "                    \"xdf_20211218_cle_bal_noCnoA\"+\\\n",
    "                    str(upsample_ratio)+\\\n",
    "                    \"_normal\"+\\\n",
    "                    \".csv\"\n",
    "\n",
    "Xdf.to_csv(path_2save + filename_to_save, index=False)\n",
    "\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\")\n",
    "print(f\"The filename is:\\n    {filename_to_save}\\n\")\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyAIsense_env",
   "language": "python",
   "name": "pyaisense"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
