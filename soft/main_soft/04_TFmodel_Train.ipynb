{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## environment preparation\n",
    "##============================\n",
    "\n",
    "%reset -f\n",
    "\n",
    "%connect_info #might be used for \"jupyter-console\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "##============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, Reshape, MaxPooling1D, MaxPooling2D, BatchNormalization, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings and Paths\n",
    "##============================\n",
    "\n",
    "path_ipynb = os.getcwd()\n",
    "path_base = path_ipynb + \"/../../data/\"\n",
    "\n",
    "## the filename with data is defined and presented at the end of\n",
    "## \"03_Data_Balance_Upsample_Save_to_Single_File.ipynb\"\n",
    "path_add_folder_with_data = \"processed_data/\"\n",
    "filename2load = \"demo_run_xdf_20211218_cle_bal_noCnoA1_normal.csv\"\n",
    "\n",
    "path_add_folder2savemodel = \"tf_models/\"\n",
    "model_filename = 'model_demo_run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "##============================\n",
    "\n",
    "Xdf = pd.read_csv(path_base + path_add_folder_with_data + filename2load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split to Data and Labels\n",
    "##============================\n",
    "\n",
    "X_init = Xdf.iloc[:,1:7]\n",
    "Y_init = Xdf.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets see the classes\n",
    "##============================\n",
    "\n",
    "classes_values = Xdf.label.unique().tolist()\n",
    "classes_values.sort()\n",
    "classes = len(classes_values)\n",
    "classes_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary to keep labels\n",
    "##============================\n",
    "\n",
    "dict_4labels = dict.fromkeys(classes_values, \"default\")\n",
    "d_counter = 0\n",
    "for d_key in dict_4labels:\n",
    "    d_counter += 1\n",
    "    dict_4labels[d_key]  = d_counter\n",
    "\n",
    "dict_4labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data converting and reshaping for training\n",
    "##============================\n",
    "\n",
    "\n",
    "# Set random seeds for repeatable results\n",
    "RANDOM_SEED = 3\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y = tf.keras.utils.to_categorical(Y_init.replace(dict_4labels)-1, classes)\n",
    "X = X_init.to_numpy()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "input_length = X_train[0].shape[0]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "callbacks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a model and compile it\n",
    "##============================\n",
    "\n",
    "\n",
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(36, \n",
    "                activation='relu', \n",
    "                activity_regularizer=tf.keras.regularizers.l1(0.00001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(24, \n",
    "                activation='relu',\n",
    "                activity_regularizer=tf.keras.regularizers.l1(0.00001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(classes, \n",
    "                activation='softmax', \n",
    "                name='y_pred'))\n",
    "\n",
    "# this controls the learning rate\n",
    "opt = Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# this controls the batch size, or you can manipulate the tf.data.Dataset objects yourself\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "# train the neural network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Fit! \n",
    "## The training process is here. \n",
    "##============================\n",
    "\n",
    "model.fit(train_dataset, epochs=15, validation_data=validation_dataset, verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets see the model summary\n",
    "##============================\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "##============================\n",
    "\n",
    "loss, acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(\"Model, accuracy: {:5.2f}%\".format(100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model to disk\n",
    "##============================\n",
    "\n",
    "# model.save('saved_model')\n",
    "# !mkdir -p saved_model\n",
    "\n",
    "model.save(path_base + path_add_folder2savemodel + model_filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross check that the model is saved correctly\n",
    "##============================\n",
    "\n",
    "\n",
    "## Load into a new model\n",
    "new_model = tf.keras.models.load_model(path_base + path_add_folder2savemodel + model_filename )\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-evaluate the model\n",
    "##============================\n",
    "\n",
    "loss, acc = new_model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "print(new_model.predict(X_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "##### Some tests to cross-check the model\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### random sample from TRAIN set\n",
    "randt_index_X = np.random.randint(low=0, high=X_train.shape[0], size=1, dtype=int)\n",
    "print(f\"Values_Train: {X_train[randt_index_X,:]}\")\n",
    "print(f\"Label_Train:  {Y_train[randt_index_X]}\")\n",
    "print(f\"Label_Text:   {pd.DataFrame(classes_values).loc[[bool(cc) for cc in Y_train[randt_index_X].tolist()[0]],:].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### random sample from TEST set\n",
    "randt_index_X = np.random.randint(low=0, high=X_test.shape[0], size=1, dtype=int)\n",
    "print(f\"Values_Test: {X_test[randt_index_X,:]}\")\n",
    "print(f\"Label_Test:  {Y_test[randt_index_X]}\")\n",
    "print(f\"Label_Text:   {pd.DataFrame(classes_values).loc[[bool(cc) for cc in Y_test[randt_index_X].tolist()[0]],:].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### random sample from TEST set\n",
    "randt_index_X = np.random.randint(low=0, high=X_test.shape[0], size=1, dtype=int)\n",
    "print(f\"Values_Test: {X_test[randt_index_X,:]}\")\n",
    "print(f\"Label_Test:  {Y_test[randt_index_X]}\")\n",
    "print(f\"Label_Text:   {pd.DataFrame(classes_values).loc[[bool(cc) for cc in Y_test[randt_index_X].tolist()[0]],:].values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randt_index_X = np.random.randint(low=0, high=X_init.shape[0], size=1, dtype=int)\n",
    "\n",
    "print(f\"X sample: {X_init.loc[randt_index_X, :]} \\nY label: {Y_init.loc[randt_index_X]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyAIsense_env",
   "language": "python",
   "name": "pyaisense"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
